
@article{pan_cosmological_2020,
	title = {Cosmological parameter estimation from large-scale structure deep learning},
	url = {http://arxiv.org/abs/1908.10590},
	abstract = {We propose a light-weight deep convolutional neural network (CNN) to estimate the cosmological parameters from simulated 3-dimensional dark matter distributions with high accuracy. The training set is based on 465 realizations of a cubic box with a side length of \$256{\textbackslash} h{\textasciicircum}\{-1\}{\textbackslash} {\textbackslash}rm Mpc\$, sampled with \$128{\textasciicircum}3\$ particles interpolated over a cubic grid of \$128{\textasciicircum}3\$ voxels. These volumes have cosmological parameters varying within the flat \${\textbackslash}Lambda\$CDM parameter space of \$0.16 {\textbackslash}leq {\textbackslash}Omega\_m {\textbackslash}leq 0.46\$ and \$2.0 {\textbackslash}leq 10{\textasciicircum}9 A\_s {\textbackslash}leq 2.3\$. The neural network takes as an input cubes with \$32{\textasciicircum}3\$ voxels and has three convolution layers, three dense layers, together with some batch normalization and pooling layers. In the final predictions from the network we find a \$2.5{\textbackslash}\%\$ bias on the primordial amplitude \${\textbackslash}sigma\_8\$ that can not easily be resolved by continued training. We correct this bias to obtain unprecedented accuracy in the cosmological parameter estimation with statistical uncertainties of \${\textbackslash}delta {\textbackslash}Omega\_m\$=0.0015 and \${\textbackslash}delta {\textbackslash}sigma\_8\$=0.0029, which are several times better than the results of previous CNN works. Compared with a 2-point analysis method using clustering region of 0-130 and 10-130 \$h{\textasciicircum}\{-1\}\$ Mpc, the CNN constraints are several times and an order of magnitude more precise, respectively. Finally, we conduct preliminary checks of the error-tolerance abilities of the neural network, and find that it exhibits robustness against smoothing, masking, random noise, global variation, rotation, reflection, and simulation resolution. Those effects are well understood in typical clustering analysis, but had not been tested before for the CNN approach. Our work shows that CNN can be more promising than people expected in deriving tight cosmological constraints from the cosmic large scale structure.},
	urldate = {2020-10-02},
	journal = {arXiv:1908.10590 [astro-ph, physics:gr-qc]},
	author = {Pan, Shuyang and Liu, Miaoxin and Forero-Romero, Jaime and Sabiu, Cristiano G. and Li, Zhigang and Miao, Haitao and Li, Xiao-Dong},
	month = jun,
	year = {2020},
	note = {arXiv: 1908.10590},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, General Relativity and Quantum Cosmology},
	annote = {Comment: 17 pages, 10 figures, 1 table},
}

@article{alsing_fast_2019,
	title = {Fast likelihood-free cosmology with neural density estimators and active learning},
	issn = {0035-8711, 1365-2966},
	url = {http://arxiv.org/abs/1903.00007},
	doi = {10.1093/mnras/stz1960},
	abstract = {Likelihood-free inference provides a framework for performing rigorous Bayesian inference using only forward simulations, properly accounting for all physical and observational effects that can be successfully included in the simulations. The key challenge for likelihood-free applications in cosmology, where simulation is typically expensive, is developing methods that can achieve high-fidelity posterior inference with as few simulations as possible. Density-estimation likelihood-free inference (DELFI) methods turn inference into a density estimation task on a set of simulated data-parameter pairs, and give orders of magnitude improvements over traditional Approximate Bayesian Computation approaches to likelihood-free inference. In this paper we use neural density estimators (NDEs) to learn the likelihood function from a set of simulated datasets, with active learning to adaptively acquire simulations in the most relevant regions of parameter space on-the-fly. We demonstrate the approach on a number of cosmological case studies, showing that for typical problems high-fidelity posterior inference can be achieved with just \${\textbackslash}mathcal\{O\}(10{\textasciicircum}3)\$ simulations or fewer. In addition to enabling efficient simulation-based inference, for simple problems where the form of the likelihood is known, DELFI offers a fast alternative to MCMC sampling, giving orders of magnitude speed-up in some cases. Finally, we introduce {\textbackslash}textsc\{pydelfi\} -- a flexible public implementation of DELFI with NDEs and active learning -- available at {\textbackslash}url\{https://github.com/justinalsing/pydelfi\}.},
	urldate = {2021-03-15},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Alsing, Justin and Charnock, Tom and Feeney, Stephen and Wandelt, Benjamin},
	month = jul,
	year = {2019},
	note = {arXiv: 1903.00007},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
	pages = {stz1960},
	annote = {Comment: Submitted to MNRAS Feb 2019},
}

@article{wagner-carena_hierarchical_2021,
	title = {Hierarchical {Inference} {With} {Bayesian} {Neural} {Networks}: {An} {Application} to {Strong} {Gravitational} {Lensing}},
	volume = {909},
	issn = {0004-637X, 1538-4357},
	shorttitle = {Hierarchical {Inference} {With} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.13787},
	doi = {10.3847/1538-4357/abdf59},
	abstract = {In the past few years, approximate Bayesian Neural Networks (BNNs) have demonstrated the ability to produce statistically consistent posteriors on a wide range of inference problems at unprecedented speed and scale. However, any disconnect between training sets and the distribution of real-world objects can introduce bias when BNNs are applied to data. This is a common challenge in astrophysics and cosmology, where the unknown distribution of objects in our Universe is often the science goal. In this work, we incorporate BNNs with flexible posterior parameterizations into a hierarchical inference framework that allows for the reconstruction of population hyperparameters and removes the bias introduced by the training distribution. We focus on the challenge of producing posterior PDFs for strong gravitational lens mass model parameters given Hubble Space Telescope (HST) quality single-filter, lens-subtracted, synthetic imaging data. We show that the posterior PDFs are sufficiently accurate (i.e., statistically consistent with the truth) across a wide variety of power-law elliptical lens mass distributions. We then apply our approach to test data sets whose lens parameters are drawn from distributions that are drastically different from the training set. We show that our hierarchical inference framework mitigates the bias introduced by an unrepresentative training set's interim prior. Simultaneously, given a sufficiently broad training set, we can precisely reconstruct the population hyperparameters governing our test distributions. Our full pipeline, from training to hierarchical inference on thousands of lenses, can be run in a day. The framework presented here will allow us to efficiently exploit the full constraining power of future ground- and space-based surveys.},
	number = {2},
	urldate = {2021-03-24},
	journal = {The Astrophysical Journal},
	author = {Wagner-Carena, Sebastian and Park, Ji Won and Birrer, Simon and Marshall, Philip J. and Roodman, Aaron and Wechsler, Risa H.},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.13787},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
	pages = {187},
	annote = {Comment: Accepted by ApJ. Code available at https://github.com/swagnercarena/ovejero},
}

@article{cranmer_frontier_2020,
	title = {The frontier of simulation-based inference},
	url = {http://arxiv.org/abs/1911.01429},
	abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving new momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound change these developments may have on science.},
	urldate = {2021-09-14},
	journal = {arXiv:1911.01429 [cs, stat]},
	author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	month = apr,
	year = {2020},
	note = {arXiv: 1911.01429},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: 10 pages, 3 figures, proceedings for the Sackler Colloquia at the US National Academy of Sciences. v2: fixed typos. v3: clarified text, added references},
}

@article{villaescusa-navarro_multifield_2021,
	title = {Multifield {Cosmology} with {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2109.09747},
	abstract = {Astrophysical processes such as feedback from supernovae and active galactic nuclei modify the properties and spatial distribution of dark matter, gas, and galaxies in a poorly understood way. This uncertainty is one of the main theoretical obstacles to extract information from cosmological surveys. We use 2,000 state-of-the-art hydrodynamic simulations from the CAMELS project spanning a wide variety of cosmological and astrophysical models and generate hundreds of thousands of 2-dimensional maps for 13 different fields: from dark matter to gas and stellar properties. We use these maps to train convolutional neural networks to extract the maximum amount of cosmological information while marginalizing over astrophysical effects at the field level. Although our maps only cover a small area of \$(25{\textasciitilde}h{\textasciicircum}\{-1\}\{{\textbackslash}rm Mpc\}){\textasciicircum}2\$, and the different fields are contaminated by astrophysical effects in very different ways, our networks can infer the values of \${\textbackslash}Omega\_\{{\textbackslash}rm m\}\$ and \${\textbackslash}sigma\_8\$ with a few percent level precision for most of the fields. We find that the marginalization performed by the network retains a wealth of cosmological information compared to a model trained on maps from gravity-only N-body simulations that are not contaminated by astrophysical effects. Finally, we train our networks on multifields -- 2D maps that contain several fields as different colors or channels -- and find that not only they can infer the value of all parameters with higher accuracy than networks trained on individual fields, but they can constrain the value of \${\textbackslash}Omega\_\{{\textbackslash}rm m\}\$ with higher accuracy than the maps from the N-body simulations.},
	urldate = {2021-09-22},
	journal = {arXiv:2109.09747 [astro-ph]},
	author = {Villaescusa-Navarro, Francisco and Anglés-Alcázar, Daniel and Genel, Shy and Spergel, David N. and Li, Yin and Wandelt, Benjamin and Nicola, Andrina and Thiele, Leander and Hassan, Sultan and Matilla, Jose Manuel Zorrilla and Narayanan, Desika and Dave, Romeel and Vogelsberger, Mark},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.09747},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 11 pages, 7 figures. First paper of a series of four. All 2D maps, codes, and networks weights publicly available at https://camels-multifield-dataset.readthedocs.io},
}

@article{tam_likelihood-free_2021,
	title = {Likelihood-free {Forward} {Modeling} for {Cluster} {Weak} {Lensing} and {Cosmology}},
	url = {http://arxiv.org/abs/2109.09741},
	abstract = {Likelihood-free inference provides a rigorous approach to preform Bayesian analysis using forward simulations only. The main advantage of likelihood-free methods is its ability to account for complex physical processes and observational effects in forward simulations. Here we explore the potential of likelihood-free forward modeling for Bayesian cosmological inference using the redshift evolution of the cluster abundance combined with weak-lensing mass calibration. We use two complementary likelihood-free methods, namely Approximate Bayesian Computation (ABC) and Density-Estimation Likelihood-Free Inference (DELFI), to develop an analysis procedure for inference of the cosmological parameters \$({\textbackslash}Omega\_{\textbackslash}mathrm\{m\},{\textbackslash}sigma\_8)\$ and the mass scale of the survey sample. Adopting an eROSITA-like selection function and a 10-percent scatter in the observable-mass relation in a flat \${\textbackslash}Lambda\$CDM cosmology with \${\textbackslash}Omega\_{\textbackslash}mathrm\{m\}=0.286\$ and \${\textbackslash}sigma\_8=0.82\$, we create a synthetic catalog of observable-selected NFW clusters in a survey area of 50 deg\${\textasciicircum}2\$. The stacked tangential shear profile and the number counts in redshift bins are used as summary statistics for both methods. By performing a series of forward simulations, we obtain convergent solutions for the posterior distribution from both methods. We find that ABC recovers broader posteriors than DELFI, especially for the \${\textbackslash}Omega\_{\textbackslash}mathrm\{m\}\$ parameter. For a weak-lensing survey with a source density of \$n\_{\textbackslash}mathrm\{g\}=20\$ arcmin\${\textasciicircum}\{-2\}\$, we obtain posterior constraints on \$S\_8={\textbackslash}sigma\_8({\textbackslash}Omega\_{\textbackslash}mathrm\{m\}/0.3){\textasciicircum}\{0.3\}\$ of \$0.836 {\textbackslash}pm 0.032\$ and \$0.810 {\textbackslash}pm 0.019\$ from ABC and DELFI, respectively. The analysis framework developed in this study will be particularly powerful for cosmological inference with ongoing cluster cosmology programs, such as the XMM-XXL survey and the eROSITA all-sky survey, in combination with wide-field weak-lensing surveys.},
	urldate = {2021-09-22},
	journal = {arXiv:2109.09741 [astro-ph]},
	author = {Tam, Sut-Ieng and Umetsu, Keiichi and Amara, Adam},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.09741},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
	annote = {Comment: Submitted to ApJ; 14 pages, 11 figures},
}

@article{villaescusa-navarro_robust_2021,
	title = {Robust marginalization of baryonic effects for cosmological inference at the field level},
	url = {http://arxiv.org/abs/2109.10360},
	abstract = {We train neural networks to perform likelihood-free inference from \$(25{\textbackslash},h{\textasciicircum}\{-1\}\{{\textbackslash}rm Mpc\}){\textasciicircum}2\$ 2D maps containing the total mass surface density from thousands of hydrodynamic simulations of the CAMELS project. We show that the networks can extract information beyond one-point functions and power spectra from all resolved scales (\${\textbackslash}gtrsim 100{\textbackslash},h{\textasciicircum}\{-1\}\{{\textbackslash}rm kpc\}\$) while performing a robust marginalization over baryonic physics at the field level: the model can infer the value of \${\textbackslash}Omega\_\{{\textbackslash}rm m\} ({\textbackslash}pm 4{\textbackslash}\%)\$ and \${\textbackslash}sigma\_8 ({\textbackslash}pm 2.5{\textbackslash}\%)\$ from simulations completely different to the ones used to train it.},
	urldate = {2021-09-23},
	journal = {arXiv:2109.10360 [astro-ph]},
	author = {Villaescusa-Navarro, Francisco and Genel, Shy and Angles-Alcazar, Daniel and Spergel, David N. and Li, Yin and Wandelt, Benjamin and Thiele, Leander and Nicola, Andrina and Matilla, Jose Manuel Zorrilla and Shao, Helen and Hassan, Sultan and Narayanan, Desika and Dave, Romeel and Vogelsberger, Mark},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.10360},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 7 pages, 4 figures. Second paper of a series of four. The 2D maps, codes, and network weights used in this paper are publicly available at https://camels-multifield-dataset.readthedocs.io},
}

@article{lu_simultaneously_2021,
	title = {Simultaneously constraining cosmology and baryonic physics via deep learning from weak lensing},
	url = {http://arxiv.org/abs/2109.11060},
	abstract = {Ongoing and planned weak lensing (WL) surveys are becoming deep enough to contain information on angular scales down to a few arcmin. To fully extract information from these small scales, we must capture non-Gaussian features in the cosmological WL signal while accurately accounting for baryonic effects. In this work, we account for baryonic physics via a baryonic correction model that modifies the matter distribution in dark matter-only \$N\$-body simulations, mimicking the effects of galaxy formation and feedback. We implement this model in a large suite of ray-tracing simulations, spanning a grid of cosmological models in \${\textbackslash}Omega\_{\textbackslash}mathrm\{m\}-{\textbackslash}sigma\_8\$ space. We then develop a convolutional neural network (CNN) architecture to learn and constrain cosmological and baryonic parameters simultaneously from the simulated WL convergence maps. We find that in a Hyper-Suprime Cam (HSC)-like survey, our CNN achieves a 1.7\${\textbackslash}times\$ tighter constraint in \${\textbackslash}Omega\_{\textbackslash}mathrm\{m\}-{\textbackslash}sigma\_8\$ space (\$1{\textbackslash}sigma\$ area) than the power spectrum and 2.1\${\textbackslash}times\$ tighter than the peak counts, showing that the CNN can efficiently extract non-Gaussian cosmological information even while marginalizing over baryonic effects. When we combine our CNN with the power spectrum, the baryonic effects degrade the constraint in \${\textbackslash}Omega\_{\textbackslash}mathrm\{m\}-{\textbackslash}sigma\_8\$ space by a factor of 2.4, compared to the much worse degradation by a factor of 4.7 or 3.7 from either method alone.},
	urldate = {2021-09-24},
	journal = {arXiv:2109.11060 [astro-ph]},
	author = {Lu, Tianhuan and Haiman, Zoltán and Matilla, José Manuel Zorrilla},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.11060},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
	annote = {Comment: 12 pages, 6 figures},
}

@article{hermans_likelihood-free_2020,
	title = {Likelihood-free {MCMC} with {Amortized} {Approximate} {Ratio} {Estimators}},
	url = {http://arxiv.org/abs/1903.04057},
	abstract = {Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to make use of approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model. We achieve this by learning a flexible amortized estimator which approximates the likelihood-to-evidence ratio. We demonstrate that the learned ratio estimator can be embedded in MCMC samplers to approximate likelihood-ratios between consecutive states in the Markov chain, allowing us to draw samples from the intractable posterior. Techniques are presented to improve the numerical stability and to measure the quality of an approximation. The accuracy of our approach is demonstrated on a variety of benchmarks against well-established techniques. Scientific applications in physics show its applicability.},
	urldate = {2021-10-06},
	journal = {arXiv:1903.04057 [cs, stat]},
	author = {Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
	month = jun,
	year = {2020},
	note = {arXiv: 1903.04057},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: v5: Camera-ready version presented at ICML 2020},
}

@article{papamakarios_sequential_2019,
	title = {Sequential {Neural} {Likelihood}: {Fast} {Likelihood}-free {Inference} with {Autoregressive} {Flows}},
	shorttitle = {Sequential {Neural} {Likelihood}},
	url = {http://arxiv.org/abs/1805.07226},
	abstract = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.},
	urldate = {2021-10-06},
	journal = {arXiv:1805.07226 [cs, stat]},
	author = {Papamakarios, George and Sterratt, David C. and Murray, Iain},
	month = jan,
	year = {2019},
	note = {arXiv: 1805.07226},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted for publication at AISTATS 2019},
}

@article{durkan_sequential_2018,
	title = {Sequential {Neural} {Methods} for {Likelihood}-free {Inference}},
	url = {http://arxiv.org/abs/1811.08723},
	abstract = {Likelihood-free inference refers to inference when a likelihood function cannot be explicitly evaluated, which is often the case for models based on simulators. Most of the literature is based on sample-based `Approximate Bayesian Computation' methods, but recent work suggests that approaches based on deep neural conditional density estimators can obtain state-of-the-art results with fewer simulations. The neural approaches vary in how they choose which simulations to run and what they learn: an approximate posterior or a surrogate likelihood. This work provides some direct controlled comparisons between these choices.},
	urldate = {2021-10-06},
	journal = {arXiv:1811.08723 [cs, stat]},
	author = {Durkan, Conor and Papamakarios, George and Murray, Iain},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.08723},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mishra-sharma_inferring_2021,
	title = {Inferring dark matter substructure with astrometric lensing beyond the power spectrum},
	url = {http://arxiv.org/abs/2110.01620},
	abstract = {Astrometry -- the precise measurement of positions and motions of celestial objects -- has emerged as a promising avenue for characterizing the dark matter population in our Galaxy. By leveraging recent advances in simulation-based inference and neural network architectures, we introduce a novel method to search for global dark matter-induced gravitational lensing signatures in astrometric datasets. Our method based on neural likelihood-ratio estimation shows significantly enhanced sensitivity to a cold dark matter population and more favorable scaling with measurement noise compared to existing approaches based on two-point correlation statistics, establishing machine learning as a powerful tool for characterizing dark matter using astrometric data.},
	urldate = {2021-10-06},
	journal = {arXiv:2110.01620 [astro-ph, physics:hep-ph]},
	author = {Mishra-Sharma, Siddharth},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.01620},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning, High Energy Physics - Phenomenology},
	annote = {Comment: 10 pages, 3 figures, extended version of paper submitted to the Machine Learning and the Physical Sciences workshop at NeurIPS 2021},
}

@article{mishra-sharma_neural_2021,
	title = {A neural simulation-based inference approach for characterizing the {Galactic} {Center} \${\textbackslash}gamma\$-ray excess},
	url = {http://arxiv.org/abs/2110.06931},
	abstract = {The nature of the Fermi gamma-ray Galactic Center Excess (GCE) has remained a persistent mystery for over a decade. Although the excess is broadly compatible with emission expected due to dark matter annihilation, an explanation in terms of a population of unresolved astrophysical point sources e.g., millisecond pulsars, remains viable. The effort to uncover the origin of the GCE is hampered in particular by an incomplete understanding of diffuse emission of Galactic origin. This can lead to spurious features that make it difficult to robustly differentiate smooth emission, as expected for a dark matter origin, from more "clumpy" emission expected for a population of relatively bright, unresolved point sources. We use recent advancements in the field of simulation-based inference, in particular density estimation techniques using normalizing flows, in order to characterize the contribution of modeled components, including unresolved point source populations, to the GCE. Compared to traditional techniques based on the statistical distribution of photon counts, our machine learning-based method is able to utilize more of the information contained in a given model of the Galactic Center emission, and in particular can perform posterior parameter estimation while accounting for pixel-to-pixel spatial correlations in the gamma-ray map. This makes the method demonstrably more resilient to certain forms of model misspecification. On application to Fermi data, the method generically attributes a smaller fraction of the GCE flux to unresolved point sources when compared to traditional approaches. We nevertheless infer such a contribution to make up a non-negligible fraction of the GCE across all analysis variations considered, with at least \$38{\textasciicircum}\{+9\}\_\{-19\}{\textbackslash}\%\$ of the excess attributed to unresolved points sources in our baseline analysis.},
	urldate = {2021-10-15},
	journal = {arXiv:2110.06931 [astro-ph, physics:hep-ph]},
	author = {Mishra-Sharma, Siddharth and Cranmer, Kyle},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.06931},
	keywords = {Astrophysics - High Energy Astrophysical Phenomena, Computer Science - Machine Learning, High Energy Physics - Phenomenology},
	annote = {Comment: 20+3 pages, 10+4 figures},
}

@article{miller_simulation-efficient_2020,
	title = {Simulation-efficient marginal posterior estimation with swyft: stop wasting your precious time},
	shorttitle = {Simulation-efficient marginal posterior estimation with swyft},
	url = {http://arxiv.org/abs/2011.13951},
	abstract = {We present algorithms (a) for nested neural likelihood-to-evidence ratio estimation, and (b) for simulation reuse via an inhomogeneous Poisson point process cache of parameters and corresponding simulations. Together, these algorithms enable automatic and extremely simulator efficient estimation of marginal and joint posteriors. The algorithms are applicable to a wide range of physics and astronomy problems and typically offer an order of magnitude better simulator efficiency than traditional likelihood-based sampling methods. Our approach is an example of likelihood-free inference, thus it is also applicable to simulators which do not offer a tractable likelihood function. Simulator runs are never rejected and can be automatically reused in future analysis. As functional prototype implementation we provide the open-source software package swyft.},
	urldate = {2021-10-21},
	journal = {arXiv:2011.13951 [astro-ph, physics:hep-ph]},
	author = {Miller, Benjamin Kurt and Cole, Alex and Louppe, Gilles and Weniger, Christoph},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.13951},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, High Energy Physics - Phenomenology},
	annote = {Comment: Accepted at Machine Learning and the Physical Sciences at NeurIPS 2020. Package: https://github.com/undark-lab/swyft/},
}

@article{cole_fast_2021,
	title = {Fast and {Credible} {Likelihood}-{Free} {Cosmology} with {Truncated} {Marginal} {Neural} {Ratio} {Estimation}},
	url = {http://arxiv.org/abs/2111.08030},
	abstract = {Sampling-based inference techniques are central to modern cosmological data analysis; these methods, however, scale poorly with dimensionality and typically require approximate or intractable likelihoods. In this paper we describe how Truncated Marginal Neural Ratio Estimation (TMNRE) (a new approach in so-called simulation-based inference) naturally evades these issues, improving the \$(i)\$ efficiency, \$(ii)\$ scalability, and \$(iii)\$ trustworthiness of the inferred posteriors. Using measurements of the Cosmic Microwave Background (CMB), we show that TMNRE can achieve converged posteriors using orders of magnitude fewer simulator calls than conventional Markov Chain Monte Carlo (MCMC) methods. Remarkably, the required number of samples is effectively independent of the number of nuisance parameters. In addition, a property called {\textbackslash}emph\{local amortization\} allows the performance of rigorous statistical consistency checks that are not accessible to sampling-based methods. TMNRE promises to become a powerful tool for cosmological data analysis, particularly in the context of extended cosmologies, where the timescale required for conventional sampling-based inference methods to converge can greatly exceed that of simple cosmological models such as \${\textbackslash}Lambda\$CDM. To perform these computations, we use an implementation of TMNRE via the open-source code {\textbackslash}texttt\{swyft\}.},
	urldate = {2021-11-17},
	journal = {arXiv:2111.08030 [astro-ph]},
	author = {Cole, Alex and Miller, Benjamin Kurt and Witte, Samuel J. and Cai, Maxwell X. and Grootes, Meiert W. and Nattino, Francesco and Weniger, Christoph},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.08030},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
	annote = {Comment: 37 pages, 13 figures. {\textbackslash}texttt\{swyft\} is available at https://github.com/undark-lab/swyft, and demonstration code for cosmological examples is available at https://github.com/acole1221/swyft-CMB},
}

@article{rhea_updates_2021,
	title = {Updates to {LUCI}: {A} {New} {Fitting} {Paradigm} {Using} {Mixture} {Density} {Networks}},
	shorttitle = {Updates to {LUCI}},
	url = {http://arxiv.org/abs/2111.12755},
	abstract = {LUCI is an general-purpose spectral line-fitting pipeline which natively integrates machine learning algorithms to initialize fit functions. LUCI currently uses point-estimates obtained from a convolutional neural network (CNN) to inform optimization algorithms; this methodology has shown great promise by reducing computation time and reducing the chance of falling into a local minimum using convex optimization methods. In this update to LUCI, we expand upon the CNN developed in Rhea et al. 2020 so that it outputs Gaussian posterior distributions of the fit parameters of interest (the velocity and broadening) rather than simple point-estimates. Moreover, these posteriors are then used to inform the priors in a Bayesian inference scheme, either emcee or dynesty. The code is publicly available at https://github.com/crhea93/LUCI.},
	urldate = {2021-11-29},
	journal = {arXiv:2111.12755 [astro-ph]},
	author = {Rhea, Carter L. and Hlavacek-Larrondo, Julie and Rousseau-Nepton, Laurie and Prunet, Simon},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.12755},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics},
	annote = {Comment: Submitted to RNAAS. Code can be found at https://github.com/crhea93/LUCI},
}

@article{kilbinger_sidestepping_2021,
	title = {Sidestepping the inversion of the weak-lensing covariance matrix with {Approximate} {Bayesian} {Computation}},
	url = {http://arxiv.org/abs/2112.03148},
	abstract = {Weak gravitational lensing is one of the few direct methods to map the dark-matter distribution on large scales in the Universe, and to estimate cosmological parameters. We study a Bayesian inference problem where the data covariance \${\textbackslash}mathbf\{C\}\$, estimated from a number \$n\_\{{\textbackslash}textrm\{s\}\}\$ of numerical simulations, is singular. In a cosmological context of large-scale structure observations, the creation of a large number of such \$N\$-body simulations is often prohibitively expensive. Inference based on a likelihood function often includes a precision matrix, \${\textbackslash}Psi = {\textbackslash}mathbf\{C\}{\textasciicircum}\{-1\}\$. The covariance matrix corresponding to a \$p\$-dimensional data vector is singular for \$p {\textbackslash}ge n\_\{{\textbackslash}textrm\{s\}\}\$, in which case the precision matrix is unavailable. We propose the likelihood-free inference method Approximate Bayesian Computation (ABC) as a solution that circumvents the inversion of the singular covariance matrix. We present examples of increasing degree of complexity, culminating in a realistic cosmological scenario of the determination of the weak-gravitational lensing power spectrum for the upcoming European Space Agency satellite Euclid. While we found the ABC parameter estimate variances to be mildly larger compared to likelihood-based approaches, which are restricted to settings with \$p {\textless} n\_\{{\textbackslash}textrm\{s\}\}\$, we obtain unbiased parameter estimates with ABC even in extreme cases where \$p / n\_\{{\textbackslash}textrm\{s\}\} {\textbackslash}gg 1\$. The code has been made publicly available to ensure the reproducibility of the results.},
	urldate = {2021-12-07},
	journal = {arXiv:2112.03148 [astro-ph]},
	author = {Kilbinger, Martin and Ishida, Emille E. O. and Cisewski-Kehe, Jessi},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.03148},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
	annote = {Comment: 24 pages, 8 figures, 3 tables. Submitted to the Annals of Applied Statistics. Code publicly available at https://github.com/emilleishida/CorrMatrix\_ABC},
}

@article{legin_simulation-based_2021,
	title = {Simulation-{Based} {Inference} of {Strong} {Gravitational} {Lensing} {Parameters}},
	url = {http://arxiv.org/abs/2112.05278},
	abstract = {In the coming years, a new generation of sky surveys, in particular, Euclid Space Telescope (2022), and the Rubin Observatory's Legacy Survey of Space and Time (LSST, 2023) will discover more than 200,000 new strong gravitational lenses, which represents an increase of more than two orders of magnitude compared to currently known sample sizes. Accurate and fast analysis of such large volumes of data under a statistical framework is therefore crucial for all sciences enabled by strong lensing. Here, we report on the application of simulation-based inference methods, in particular, density estimation techniques, to the predictions of the set of parameters of strong lensing systems from neural networks. This allows us to explicitly impose desired priors on lensing parameters, while guaranteeing convergence to the optimal posterior in the limit of perfect performance.},
	urldate = {2021-12-15},
	journal = {arXiv:2112.05278 [astro-ph]},
	author = {Legin, Ronan and Hezaveh, Yashar and Levasseur, Laurence Perreault and Wandelt, Benjamin},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.05278},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
}

@article{hortua_constraining_2021,
	title = {Constraining cosmological parameters from {N}-body simulations with {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2112.11865},
	abstract = {In this paper, we use The Quijote simulations in order to extract the cosmological parameters through Bayesian Neural Networks. This kind of model has a remarkable ability to estimate the associated uncertainty, which is one of the ultimate goals in the precision cosmology era. We demonstrate the advantages of BNNs for extracting more complex output distributions and non-Gaussianities information from the simulations.},
	urldate = {2022-01-03},
	journal = {arXiv:2112.11865 [astro-ph, stat]},
	author = {Hortua, Hector J.},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.11865},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Statistics - Machine Learning},
	annote = {Comment: Published at NeurIPS 2021 workshop: Bayesian Deep Learning},
}

@article{villaescusa-navarro_cosmology_2022,
	title = {Cosmology with one galaxy?},
	url = {http://arxiv.org/abs/2201.02202},
	abstract = {Galaxies can be characterized by many internal properties such as stellar mass, gas metallicity, and star-formation rate. We quantify the amount of cosmological and astrophysical information that the internal properties of individual galaxies and their host dark matter halos contain. We train neural networks using hundreds of thousands of galaxies from 2,000 state-of-the-art hydrodynamic simulations with different cosmologies and astrophysical models of the CAMELS project to perform likelihood-free inference on the value of the cosmological and astrophysical parameters. We find that knowing the internal properties of a single galaxy allow our models to infer the value of \${\textbackslash}Omega\_\{{\textbackslash}rm m\}\$, at fixed \${\textbackslash}Omega\_\{{\textbackslash}rm b\}\$, with a \${\textbackslash}sim10{\textbackslash}\%\$ precision, while no constraint can be placed on \${\textbackslash}sigma\_8\$. Our results hold for any type of galaxy, central or satellite, massive or dwarf, at all considered redshifts, \$z{\textbackslash}leq3\$, and they incorporate uncertainties in astrophysics as modeled in CAMELS. However, our models are not robust to changes in subgrid physics due to the large intrinsic differences the two considered models imprint on galaxy properties. We find that the stellar mass, stellar metallicity, and maximum circular velocity are among the most important galaxy properties to determine the value of \${\textbackslash}Omega\_\{{\textbackslash}rm m\}\$. We believe that our results can be explained taking into account that changes in the value of \${\textbackslash}Omega\_\{{\textbackslash}rm m\}\$, or potentially \${\textbackslash}Omega\_\{{\textbackslash}rm b\}/{\textbackslash}Omega\_\{{\textbackslash}rm m\}\$, affect the dark matter content of galaxies. That effect leaves a distinct signature in galaxy properties to the one induced by galactic processes. Our results suggest that the low-dimensional manifold hosting galaxy properties provides a tight direct link between cosmology and astrophysics.},
	urldate = {2022-01-10},
	journal = {arXiv:2201.02202 [astro-ph]},
	author = {Villaescusa-Navarro, Francisco and Ding, Jupiter and Genel, Shy and Tonnesen, Stephanie and La Torre, Valentina and Spergel, David N. and Teyssier, Romain and Li, Yin and Heneka, Caroline and Lemos, Pablo and Anglés-Alcázar, Daniel and Nagai, Daisuke and Vogelsberger, Mark},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.02202},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
	annote = {Comment: 20+6 pages, 15 figures, data and codes to reproduce the results publicly available at https://github.com/franciscovillaescusa/Cosmo1gal},
}

@article{papamakarios_neural_2019,
	title = {Neural {Density} {Estimation} and {Likelihood}-free {Inference}},
	url = {http://arxiv.org/abs/1910.13233},
	abstract = {I consider two problems in machine learning and statistics: the problem of estimating the joint probability density of a collection of random variables, known as density estimation, and the problem of inferring model parameters when their likelihood is intractable, known as likelihood-free inference. The contribution of the thesis is a set of new methods for addressing these problems that are based on recent advances in neural networks and deep learning.},
	urldate = {2022-01-10},
	journal = {arXiv:1910.13233 [cs, stat]},
	author = {Papamakarios, George},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.13233},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: PhD thesis submitted to the University of Edinburgh in April 2019. Includes in full the following articles: arXiv:1605.06376, arXiv:1705.07057, arXiv:1805.07226},
}

@article{park_large-scale_2021,
	title = {Large-{Scale} {Gravitational} {Lens} {Modeling} with {Bayesian} {Neural} {Networks} for {Accurate} and {Precise} {Inference} of the {Hubble} {Constant}},
	volume = {910},
	issn = {0004-637X, 1538-4357},
	url = {http://arxiv.org/abs/2012.00042},
	doi = {10.3847/1538-4357/abdfc4},
	abstract = {We investigate the use of approximate Bayesian neural networks (BNNs) in modeling hundreds of time-delay gravitational lenses for Hubble constant (\$H\_0\$) determination. Our BNN was trained on synthetic HST-quality images of strongly lensed active galactic nuclei (AGN) with lens galaxy light included. The BNN can accurately characterize the posterior PDFs of model parameters governing the elliptical power-law mass profile in an external shear field. We then propagate the BNN-inferred posterior PDFs into ensemble \$H\_0\$ inference, using simulated time delay measurements from a plausible dedicated monitoring campaign. Assuming well-measured time delays and a reasonable set of priors on the environment of the lens, we achieve a median precision of \$9.3\${\textbackslash}\% per lens in the inferred \$H\_0\$. A simple combination of 200 test-set lenses results in a precision of 0.5 \${\textbackslash}textrm\{km s\}{\textasciicircum}\{-1\} {\textbackslash}textrm\{ Mpc\}{\textasciicircum}\{-1\}\$ (\$0.7{\textbackslash}\%\$), with no detectable bias in this \$H\_0\$ recovery test. The computation time for the entire pipeline -- including the training set generation, BNN training, and \$H\_0\$ inference -- translates to 9 minutes per lens on average for 200 lenses and converges to 6 minutes per lens as the sample size is increased. Being fully automated and efficient, our pipeline is a promising tool for exploring ensemble-level systematics in lens modeling for \$H\_0\$ inference.},
	number = {1},
	urldate = {2022-01-18},
	journal = {The Astrophysical Journal},
	author = {Park, Ji Won and Wagner-Carena, Sebastian and Birrer, Simon and Marshall, Philip J. and Lin, Joshua Yao-Yu and Roodman, Aaron},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.00042},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning},
	pages = {39},
	annote = {Comment: 21 pages (+2 appendix), 17 figures. Published in ApJ. Code at https://github.com/jiwoncpark/h0rton. Datasets, trained models, and inference results at https://zenodo.org/record/4300382},
}

@article{sun_alpha-deep_2022,
	title = {alpha-{Deep} {Probabilistic} {Inference} (alpha-{DPI}): efficient uncertainty quantification from exoplanet astrometry to black hole feature extraction},
	shorttitle = {alpha-{Deep} {Probabilistic} {Inference} (alpha-{DPI})},
	url = {http://arxiv.org/abs/2201.08506},
	abstract = {Inference is crucial in modern astronomical research, where hidden astrophysical features and patterns are often estimated from indirect and noisy measurements. Inferring the posterior of hidden features, conditioned on the observed measurements, is essential for understanding the uncertainty of results and downstream scientific interpretations. Traditional approaches for posterior estimation include sampling-based methods and variational inference. However, sampling-based methods are typically slow for high-dimensional inverse problems, while variational inference often lacks estimation accuracy. In this paper, we propose alpha-DPI, a deep learning framework that first learns an approximate posterior using alpha-divergence variational inference paired with a generative neural network, and then produces more accurate posterior samples through importance re-weighting of the network samples. It inherits strengths from both sampling and variational inference methods: it is fast, accurate, and scalable to high-dimensional problems. We apply our approach to two high-impact astronomical inference problems using real data: exoplanet astrometry and black hole feature extraction.},
	urldate = {2022-01-24},
	journal = {arXiv:2201.08506 [astro-ph]},
	author = {Sun, He and Bouman, Katherine L. and Tiede, Paul and Wang, Jason J. and Blunt, Sarah and Mawet, Dimitri},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.08506},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@article{dai_translation_2022,
	title = {Translation and {Rotation} {Equivariant} {Normalizing} {Flow} ({TRENF}) for {Optimal} {Cosmological} {Analysis}},
	url = {http://arxiv.org/abs/2202.05282},
	abstract = {Our universe is homogeneous and isotropic, and its perturbations obey translation and rotation symmetry. In this work we develop Translation and Rotation Equivariant Normalizing Flow (TRENF), a generative Normalizing Flow (NF) model which explicitly incorporates these symmetries, defining the data likelihood via a sequence of Fourier space-based convolutions and pixel-wise nonlinear transforms. TRENF gives direct access to the high dimensional data likelihood p(x{\textbar}y) as a function of the labels y, such as cosmological parameters. In contrast to traditional analyses based on summary statistics, the NF approach has no loss of information since it preserves the full dimensionality of the data. On Gaussian random fields, the TRENF likelihood agrees well with the analytical expression and saturates the Fisher information content in the labels y. On nonlinear cosmological overdensity fields from N-body simulations, TRENF leads to significant improvements in constraining power over the standard power spectrum summary statistic. TRENF is also a generative model of the data, and we show that TRENF samples agree well with the N-body simulations it trained on, and that the inverse mapping of the data agrees well with a Gaussian white noise both visually and on various summary statistics: when this is perfectly achieved the resulting p(x{\textbar}y) likelihood analysis becomes optimal. Finally, we develop a generalization of this model that can handle effects that break the symmetry of the data, such as the survey mask, which enables likelihood analysis on data without periodic boundaries.},
	urldate = {2022-02-14},
	journal = {arXiv:2202.05282 [astro-ph]},
	author = {Dai, Biwei and Seljak, Uros},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.05282},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Machine Learning},
	annote = {Comment: 11 pages, 10 figures. Submitted to MNRAS. Comments welcome},
}

@article{hahn_accelerated_2022,
	title = {Accelerated {Bayesian} {SED} {Modeling} using {Amortized} {Neural} {Posterior} {Estimation}},
	url = {http://arxiv.org/abs/2203.07391},
	abstract = {State-of-the-art spectral energy distribution (SED) analyses use a Bayesian framework to infer the physical properties of galaxies from observed photometry or spectra. They require sampling from a high-dimensional space of SED model parameters and take \${\textgreater}10-100\$ CPU hours per galaxy, which renders them practically infeasible for analyzing the \$billions\$ of galaxies that will be observed by upcoming galaxy surveys (\$e.g.\$ DESI, PFS, Rubin, Webb, and Roman). In this work, we present an alternative scalable approach to rigorous Bayesian inference using Amortized Neural Posterior Estimation (ANPE). ANPE is a simulation-based inference method that employs neural networks to estimate the posterior probability distribution over the full range of observations. Once trained, it requires no additional model evaluations to estimate the posterior. We present, and publicly release, \$\{{\textbackslash}rm SED\}\{flow\}\$, an ANPE method to produce posteriors of the recent Hahn et al. (2022) SED model from optical photometry. \$\{{\textbackslash}rm SED\}\{flow\}\$ takes \$\{{\textbackslash}sim\}1\$ \$second{\textasciitilde}per{\textasciitilde}galaxy\$ to obtain the posterior distributions of 12 model parameters, all of which are in excellent agreement with traditional Markov Chain Monte Carlo sampling results. We also apply \$\{{\textbackslash}rm SED\}\{flow\}\$ to 33,884 galaxies in the NASA-Sloan Atlas and publicly release their posteriors: see https://changhoonhahn.github.io/SEDflow.},
	urldate = {2022-03-16},
	journal = {arXiv:2203.07391 [astro-ph, stat]},
	author = {Hahn, ChangHoon and Melchior, Peter},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.07391},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Statistics - Machine Learning},
	annote = {Comment: 21 pages, 5 figures; submitted to ApJ; code available at https://changhoonhahn.github.io/SEDflow},
}

@article{zhao_implicit_2022,
	title = {Implicit {Likelihood} {Inference} of {Reionization} {Parameters} from the 21 cm {Power} {Spectrum}},
	url = {http://arxiv.org/abs/2203.15734},
	abstract = {The first measurements of the 21 cm brightness temperature power spectrum from the epoch of reionization will be very likely achieved in the near future by radio interferometric array experiments such as the Hydrogen Epoch of Reionization Array (HERA) and the precursors of the Square Kilometre Array (SKA). Standard MCMC analyses use an explicit likelihood approximation to infer the reionization and astrophysical parameters from the 21 cm power spectrum. In this paper, we present a new Bayesian inference of the reionization parameters where the likelihood is implicitly defined through forward simulations using density estimation likelihood-free inference (DELFI). Realistic effects including thermal noise and foreground avoidance are also applied to the mock observations from the HERA and SKA. We demonstrate that this method recovers accurate posterior distributions for the reionization parameters, and outperforms the standard MCMC analysis in terms of the location and size of credible parameter regions. With the minutes-level processing time once the network is trained, this technique is a promising approach for the scientific interpretation of future 21 cm power spectrum observation data. Our code 21cmDELFI-PS is publicly available at this link.},
	urldate = {2022-03-31},
	journal = {arXiv:2203.15734 [astro-ph]},
	author = {Zhao, Xiaosheng and Mao, Yi and Wandelt, Benjamin D.},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.15734},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
	annote = {Comment: 13 pages, 6 figures, 4 tables. Submitted to ApJ. Comments welcome},
}

@article{chen_lightweight_2022,
	title = {Lightweight starshade position sensing with convolutional neural networks and simulation-based inference},
	url = {http://arxiv.org/abs/2204.03853},
	abstract = {Starshades are a leading technology to enable the direct detection and spectroscopic characterization of Earth-like exoplanets. To keep the starshade and telescope aligned over large separations, reliable sensing of the peak of the diffracted light of the occluded star is required. Current techniques rely on image matching or model fitting, both of which put substantial computational burdens on resource-limited spacecraft computers. We present a lightweight image processing method based on a convolutional neural network paired with a simulation-based inference technique to estimate the position of the spot of Arago and its uncertainty. The method achieves an accuracy of a few centimeters across the entire pupil plane, while only requiring 1.6 MB in stored data structures and 5.3 MFLOPs (million floating point operations) per image at test time. By deploying our method at the Princeton Starshade Testbed, we demonstrate that the neural network can be trained on simulated images and used on real images, and that it can successfully be integrated in the control system for closed-loop formation flying.},
	urldate = {2022-04-11},
	journal = {arXiv:2204.03853 [astro-ph]},
	author = {Chen, Andrew and Harness, Anthony and Melchior, Peter},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.03853},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
	annote = {Comment: submitted to JATIS},
}
